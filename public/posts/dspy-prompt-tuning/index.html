<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><script>!function(e,t){var n,s,o,i;t.__SV||(window.posthog=t,t._i=[],t.init=function(a,r,c){function d(e,t){var n=t.split(".");2==n.length&&(e=e[n[0]],t=n[1]),e[t]=function(){e.push([t].concat(Array.prototype.slice.call(arguments,0)))}}(s=e.createElement("script")).type="text/javascript",s.async=!0,s.src=r.api_host.replace(".i.posthog.com","-assets.i.posthog.com")+"/static/array.js",(i=e.getElementsByTagName("script")[0]).parentNode.insertBefore(s,i);var l=t;for(void 0!==c?l=t[c]=[]:c="posthog",l.people=l.people||[],l.toString=function(e){var t="posthog";return"posthog"!==c&&(t+="."+c),e||(t+=" (stub)"),t},l.people.toString=function(){return l.toString(1)+".people (stub)"},o="capture identify alias people.set people.set_once set_config register register_once unregister opt_out_capturing has_opted_out_capturing opt_in_capturing reset isFeatureEnabled onFeatureFlags getFeatureFlag getFeatureFlagPayload reloadFeatureFlags group updateEarlyAccessFeatureEnrollment getEarlyAccessFeatures getActiveMatchingSurveys getSurveys onSessionId".split(" "),n=0;n<o.length;n++)d(l,o[n]);t._i.push([a,r,c])},t.__SV=1)}(document,window.posthog||[]),posthog.init("phc_vxLUYaSwR9EKaAuLgPH3xmn605L4wA8ChJZtmwJEgiI",{api_host:"https://us.i.posthog.com",person_profiles:"identified_only"})</script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Prompt Tuning with DSPy | Suyog's personal blog</title>
<meta name=keywords content="llm,prompt-engineering,python"><meta name=description content="What is it and why is it needed? Last week, I spent some time writing prompts for classifying documents into a fixed set of classes based on their OCR text. I spent some time tweaking and experimenting with the prompt for the model to behave exactly how I wanted it to behave. But prompt tuning, for those who have tried it, knows how painfully daunting it is to make these models behave exactly how you&rsquo;d like them to behave&mldr; that also consistently."><meta name=author content="Suyog"><link rel=canonical href=http://localhost:1313/posts/dspy-prompt-tuning/><meta name=google-site-verification content="GTM-N8FH88H3"><link crossorigin=anonymous href=/assets/css/stylesheet.4c28a66a94bdf0c82a26d82167028400897c838afe288e620ef5588afed79ea4.css integrity="sha256-TCimapS98MgqJtghZwKEAIl8g4r+KI5iDvVYiv7XnqQ=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon_io/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon_io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon_io/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/favicon_io/apple_touch_icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/dspy-prompt-tuning/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:title" content="Prompt Tuning with DSPy"><meta property="og:description" content="What is it and why is it needed? Last week, I spent some time writing prompts for classifying documents into a fixed set of classes based on their OCR text. I spent some time tweaking and experimenting with the prompt for the model to behave exactly how I wanted it to behave. But prompt tuning, for those who have tried it, knows how painfully daunting it is to make these models behave exactly how you&rsquo;d like them to behave&mldr; that also consistently."><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/posts/dspy-prompt-tuning/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-06-11T12:48:50+05:45"><meta property="article:modified_time" content="2024-06-11T12:48:50+05:45"><meta property="og:site_name" content="Suyog's personal blog"><meta name=twitter:card content="summary"><meta name=twitter:title content="Prompt Tuning with DSPy"><meta name=twitter:description content="What is it and why is it needed? Last week, I spent some time writing prompts for classifying documents into a fixed set of classes based on their OCR text. I spent some time tweaking and experimenting with the prompt for the model to behave exactly how I wanted it to behave. But prompt tuning, for those who have tried it, knows how painfully daunting it is to make these models behave exactly how you&rsquo;d like them to behave&mldr; that also consistently."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"Prompt Tuning with DSPy","item":"http://localhost:1313/posts/dspy-prompt-tuning/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Prompt Tuning with DSPy","name":"Prompt Tuning with DSPy","description":"What is it and why is it needed? Last week, I spent some time writing prompts for classifying documents into a fixed set of classes based on their OCR text. I spent some time tweaking and experimenting with the prompt for the model to behave exactly how I wanted it to behave. But prompt tuning, for those who have tried it, knows how painfully daunting it is to make these models behave exactly how you\u0026rsquo;d like them to behave\u0026hellip; that also consistently.","keywords":["llm","prompt-engineering","python"],"articleBody":"What is it and why is it needed? Last week, I spent some time writing prompts for classifying documents into a fixed set of classes based on their OCR text. I spent some time tweaking and experimenting with the prompt for the model to behave exactly how I wanted it to behave. But prompt tuning, for those who have tried it, knows how painfully daunting it is to make these models behave exactly how you’d like them to behave… that also consistently. This snippet from a YouTube video summarizes prompt engineering in a nutshell.\nSource It took me some time to tune the prompt even for such a simple task. This is not the first time I have spent my mental sanity just to make these models follow my instructions. So, with frameworks like DSPy, which promise to algorithmically optimize LM prompts rather than relying on my brute force, intuition, and trial-and-error methods, I decided to give it a try. So without further ado, let’s dive right into it (it feels like I’m writing a script for a YouTube video, lol).\nBuilding blocks [Signatures and Modules] In DSPy, you need to write your input output expectation from LLM as dspy.Signatures. It is pretty similar to function signatures we write in python. Lets start off by writing a simple signature that defines the input and output for our use case.\nimport dspy # set the LM lm = dspy.OpenAI( model=\"gpt-3.5-turbo\", ) dspy.settings.configure(lm=lm) class DocClassificationSignature(dspy.Signature): \"\"\"Classify doc into classes\"\"\" document = dspy.InputField(desc=\"Doc data\") document_type = dspy.OutputField( desc=f\"Possible classes: invoice, bank statement, tax form, certificate of liability, other\" ) I deliberately wrote bad signature so that I could see how it got tuned over time. I could’ve speicified Doc as Document, Doc data as Document's OCR Data. After signature, we needed to define a dspy Module. A dspy module can have one or more dspy signatures. Taking analogy from pytorch, you can think of dspy signatures as nn.Linear that defines the input and output and dspy module as nn.Module.\nclass SimpleDocClassificationModule(dspy.Module): def __init__(self): super().__init__() self.classify_doc = dspy.Predict(DocClassificationSignature) def forward(self, document): return self.classify_doc(document=document) Lets see how this simple module works, I can invoke this module like this\ndocument = \"\"\"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean rutrum auctor turpis, ac pulvinar sapien pulvinar ac. Sed vehicula nunc ipsum, nec eleifend enim elementum et. Curabitur a odio vel lorem dictum pellentesque. Phasellus ac velit mauris. Cras congue eget quam nec euismod. Curabitur risus est, lobortis id nibh nec, lacinia aliquet purus. Sed ac elit gravida, imperdiet leo sit amet, facilisis ipsum. Praesent gravida, neque a elementum fermentum, ligula lacus imperdiet justo, quis pretium velit tellus ac est. Proin volutpat dui eget leo tincidunt, vitae mattis quam placerat. Curabitur et neque et dolor vulputate feugiat et sit amet massa.\"\"\" op = SimpleDocClassificationModule()(document) print(op) Prediction( document_type='Other' ) The module sucessfully predicted the right class for the text. Lets inspect the underlying LLM call it made to get the output\nlm.inspect_history(n=1) Classify doc into classes --- Follow the following format. Document: Doc data Document Type: Possible classes: invoice, bank statement, tax form, certificate of liability, other --- Document: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean rutrum auctor turpis, ac pulvinar sapien pulvinar ac. Sed vehicula nunc ipsum, nec eleifend enim elementum et. Curabitur a odio vel lorem dictum pellentesque. Phasellus ac velit mauris. Cras congue eget quam nec euismod. Curabitur risus est, lobortis id nibh nec, lacinia aliquet purus. Sed ac elit gravida, imperdiet leo sit amet, facilisis ipsum. Praesent gravida, neque a elementum fermentum, ligula lacus imperdiet justo, quis pretium velit tellus ac est. Proin volutpat dui eget leo tincidunt, vitae mattis quam placerat. Curabitur et neque et dolor vulputate feugiat et sit amet massa. Document Type: Other The Tuning Dataset To tune the prompt, I needed some examples to use as a training and test set. I resorted to doing what most current research in LLMs does: generating synthetic data from LLMs. Here is a short snippet I used:\nSYSTEM_PROMPT = \"You are a highly intelligent document generator that will generate a dummy ocr text for the provided document type. You will not generate any other text apart from the OCR Text\" classes = [\"invoice\", \"bank statement\", \"tax form\", \"certificate of liability\", \"other\"] generated_texts = defaultdict(list) for _class in classes: # 10 examples per class for _ in range(10): completions = client.chat.completions.create( messages=[ {\"role\": \"system\", \"content\": SYSTEM_PROMPT}, { \"role\": \"user\", \"content\": f\"Please generate dummy ocr text for document type: {_class}. Strictly give only the OCR value and no other text\", }, ], model=\"gpt-3.5-turbo\", ) generated_texts[_class].append(completions.choices[0].message.content) df = pd.DataFrame(generated_texts) In just under 5 minutes (including the time to write the above snippet), I had 50 document ocrs I could use to tune the prompt. What a wonderful era we are living in!\nNow, let’s take the 50 samples we generated and prepare them in a format DSPy can consume. For this, we need to convert our data into a list of their data structure dspy.Example, i.e., (List['dspy.Example']). Then, just like any other supervised learning task, we need to split our dataset into training and testing sets. I chose a 60-40 split—not for any specific reason but just to see how much accuracy gain we can achieve with a small number of training samples. Here is the code to accomplish everything I just mentioned:\nclasses = df.columns.to_list() dataset = [] for _class in classes: for item in df[_class].to_list(): example = dspy.Example(document=item, document_type=_class).with_inputs(\"document\") dataset.append(example) random.shuffle(dataset) train_set = dataset[:30] test_set = dataset[30:] DSPy Optimizers (prev. Teleprompters) Now lets take the above dataset we generated to tune the prompt using DSPy’s Optimizers(formerly known as teleprompters, which sounded pretty cool imo). As of this writing, they provide 8 optimizers. They have a good starter guide regarding which optimizer to use in which cases. Some optimizers add few-shot examples to the prompt, while others optimize the initial prompt, and some do both at the same time.\nI’ll be using MIPRO optimizer that will optimize the initial prompt given as well as generate few shot examples. If you want a deeper understanding of how this optimizer works under the hood, DSPy has good deep dive article about how they optimize the prompt and how they generate few shot examples.\nBut like any other traditional ML training techniques, we need to first define our evaluation metric/criteria. DSPy allows us to define our own custom function, which will receive ground truth and prediction. We can write any logic of our own and return a bool. For my use case, I used fuzzy matching to check the match between my ground truth and prediction.\nfrom rapidfuzz import fuzz def is_similar(gold:dspy.Example, pred:dspy.Example, trace=None): gt = gold.document_type prediction = pred.document_type score = fuzz.partial_ratio(gt, prediction) return score \u003e= 80 Now, I’ll create the optimizer with the metric function I just defined.\nfrom dspy.teleprompt import MIPRO teleprompter = MIPRO( metric=is_similar, verbose=True ) Finally, we can start the tunining process by calling teleprompter.compile() with necessary arguments.\nkwargs = dict(num_threads=4, display_progress=True, display_table=0) optimized_module = teleprompter.compile( SimpleDocClassificationModule(), trainset=train_set, num_trials=2, max_bootstrapped_demos=3, max_labeled_demos=3, eval_kwargs=kwargs, ) Since the optimizers use LLMs internally to tune the prompt and generate few-shot examples, teleprompter.compile() will first provide you with an estimate of the number of LLM calls it will make based on your dataset and the optimizer’s parameters that you’ve set. It’s important to carefully review this because it’s easy to misconfigure a few settings, leading the optimizer to send an astronomically high number of requests to your LLM.\nAn estimate of LLM calls that the optimizer will make So for my use case, there will be ~80 LLM calls. That in itself feels like a pretty large number of requests for such a simple usecase and small dataset. I’m a bit skeptical about the scalability of this approach as the dataset size grows. However, it is still a much more economically viable option compared to fine-tuning the model itself.\nOnce the tuning begins, we can observe how the metrics change after each iteration. Similarly with the debug mode, we can also view the prompts that are getting executed under the hood for tuning.\nExample of prompt tuning Finally, once the tuning is completed, we can take a look into the optimized program.\nprint(optimized_module) classify_doc = Predict(StringSignature(document -\u003e document_type instructions='Consider semantic features of the document to distinguish between classes' document = Field(annotation=str required=True json_schema_extra={'desc': 'Doc data', '__dspy_field_type': 'input', 'prefix': 'Document:'}) document_type = Field(annotation=str required=True json_schema_extra={'desc': 'Possible classes: invoice, bank statement, tax form, certificate of liability, other', '__dspy_field_type': 'output', 'prefix': 'Classification:'}) )) Just prompting the module doesn’t show the few shot examples it has generated. For that, lets predict on the document we defined earlier and inspect the full LM call history to see the optimized prompt and the few shot examples that the optimizer generated.\ndocument = \"\"\"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean rutrum auctor turpis, ac pulvinar sapien pulvinar ac. Sed vehicula nunc ipsum, nec eleifend enim elementum et. Curabitur a odio vel lorem dictum pellentesque. Phasellus ac velit mauris. Cras congue eget quam nec euismod. Curabitur risus est, lobortis id nibh nec, lacinia aliquet purus. Sed ac elit gravida, imperdiet leo sit amet, facilisis ipsum. Praesent gravida, neque a elementum fermentum, ligula lacus imperdiet justo, quis pretium velit tellus ac est. Proin volutpat dui eget leo tincidunt, vitae mattis quam placerat. Curabitur et neque et dolor vulputate feugiat et sit amet massa.\"\"\" op = optimized_module(document) lm.inspect_history(n=1) Consider semantic features of the document to distinguish between classes --- Follow the following format. Document: Doc data Classification: Possible classes: invoice, bank statement, tax form, certificate of liability, other --- Document: 3047-A Copy C Remove the top KID for your records. SECTION CORPDESCRIPTION $3,719.25 Name: John Doe Address: 123 Main Street, Anytown, USA Account Number: 987654321 FED ID # 12-3456789 Code: ABC123 Do not write or staple in this space 10/15 O Tax year 16 5680 2,347.89 $ Taxpayer must sign and date in blue or black ink, and enter the above information in blue or black ink, then mail to: Department of Revenue, P.O. Box 123, Anytown, USA Income: S Total tax withheld: P KP Total Payments Total from Phaelbe6 Elected to apply to next your 134.65 Underpayment penalty T 2020 Luxury Tax 0.00 48.90 $ Overpaid Refund $2,701.19 Heike Beste This is your tax liability for the tax year shown at the top of the form. Sign and return to the address shown above. Classification: tax form --- Document: OCR Text: Invoice Number: 483921 Date: 09/27/2022 Customer: John Doe Total Amount: $350.00 Classification: invoice --- Document: 0498-5672-3498 Name: John Smith SSN: 123-45-6789 Total income: $50,000 Deductions: $10,000 Taxable income: $40,000 Tax owed: $7,000 Please consult your tax advisor for accurate information. Classification: tax form --- Document: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean rutrum auctor turpis, ac pulvinar sapien pulvinar ac. Sed vehicula nunc ipsum, nec eleifend enim elementum et. Curabitur a odio vel lorem dictum pellentesque. Phasellus ac velit mauris. Cras congue eget quam nec euismod. Curabitur risus est, lobortis id nibh nec, lacinia aliquet purus. Sed ac elit gravida, imperdiet leo sit amet, facilisis ipsum. Praesent gravida, neque a elementum fermentum, ligula lacus imperdiet justo, quis pretium velit tellus ac est. Proin volutpat dui eget leo tincidunt, vitae mattis quam placerat. Curabitur et neque et dolor vulputate feugiat et sit amet massa. Classification: other Evaluation Now that we have an optimized module, we can use dspy.evaluate.Evaluate to evalute the performance. It’s also pretty straightforward to use. We just need to define the evaluator with the desired test set and other configurations. Then, we can simply pass the module and metric function to the evaluator to get the metrics.\nLets first evaluate our previous module (before optimization).\neval_score_old = evaluator(SimpleDocClassificationModule(), metric=is_similar) Metrics before optimization We can see that with our initial module, the LLM correctly predicted 17/20 (85%), which is surprisingly good for such bad prompt.\nNow, lets see how our optimized module performed:\neval_score_new = evaluator(optimized_module, metric=is_similar) Metrics after optimization Voila, the metrics improved from 85% to 90% (though it made just one more correct prediction—aah, statistics, how many ways can people find to manipulate you! :P). Though it’s not a huge improvement, please note that the data was dummy data. We improved the prompt and added a few-shot example in an algorithmic manner rather than begging the LLM to respond correctly, which I feel is pretty neat!\n","wordCount":"2060","inLanguage":"en","datePublished":"2024-06-11T12:48:50+05:45","dateModified":"2024-06-11T12:48:50+05:45","author":{"@type":"Person","name":"Suyog"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/dspy-prompt-tuning/"},"publisher":{"@type":"Organization","name":"Suyog's personal blog","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon_io/favicon-32x32.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Home (Alt + H)"><img src=http://localhost:1313/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/about/ title=about><span>about</span></a></li><li><a href=http://localhost:1313/tags/ title=tags><span>tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Prompt Tuning with DSPy</h1><div class=post-meta><span title='2024-06-11 12:48:50 +0545 +0545'>June 11, 2024</span>&nbsp;·&nbsp;10 min&nbsp;·&nbsp;<a href=/tags/llm> llm</a>&nbsp;·&nbsp;<a href=/tags/prompt-engineering> prompt-engineering</a>&nbsp;·&nbsp;<a href=/tags/python> python</a></div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#what-is-it-and-why-is-it-needed>What is it and why is it needed?</a></li><li><a href=#building-blocks-signatures-and-modules>Building blocks [Signatures and Modules]</a></li><li><a href=#the-tuning>The Tuning</a><ul><li><a href=#dataset>Dataset</a></li><li><a href=#dspy-optimizers-prev-teleprompters>DSPy Optimizers (prev. Teleprompters)</a></li></ul></li><li><a href=#evaluation>Evaluation</a></li></ul></nav></div></details></div><div class=post-content><h2 id=what-is-it-and-why-is-it-needed>What is it and why is it needed?<a hidden class=anchor aria-hidden=true href=#what-is-it-and-why-is-it-needed>#</a></h2><p>Last week, I spent some time writing prompts for classifying documents into a fixed set of classes based on their OCR text. I spent some time tweaking and experimenting with the prompt for the model to behave exactly how I wanted it to behave. But prompt tuning, for those who have tried it, knows how painfully daunting it is to make these models behave exactly how you&rsquo;d like them to behave&mldr; that also consistently. This snippet from a YouTube video summarizes prompt engineering in a nutshell.</p><div style=text-align:center><img src=/img/dspy/meme.png alt="prompt engineering in a nutshell">
<a href="https://youtu.be/41EfOY0Ldkc?si=0WWby5mPdWdkcu35" style=font-style:italic;color:#555;display:block;margin-top:-4px;margin-bottom:4px;font-size:small>Source</a></div><p>It took me some time to tune the prompt even for such a simple task. This is not the first time I have spent my mental sanity just to make these models follow my instructions. So, with frameworks like DSPy, which promise to algorithmically optimize LM prompts rather than relying on my brute force, intuition, and trial-and-error methods, I decided to give it a try. So without further ado, let&rsquo;s dive right into it (it feels like I&rsquo;m writing a script for a YouTube video, lol).</p><h2 id=building-blocks-signatures-and-modules>Building blocks [Signatures and Modules]<a hidden class=anchor aria-hidden=true href=#building-blocks-signatures-and-modules>#</a></h2><p>In DSPy, you need to write your input output expectation from LLM as <a href=https://dspy-docs.vercel.app/docs/building-blocks/signatures><code>dspy.Signatures</code></a>. It is pretty similar to function signatures we write in python. Lets start off by writing a simple signature that defines the input and output for our use case.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>dspy</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># set the LM</span>
</span></span><span class=line><span class=cl><span class=n>lm</span> <span class=o>=</span> <span class=n>dspy</span><span class=o>.</span><span class=n>OpenAI</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>=</span><span class=s2>&#34;gpt-3.5-turbo&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>dspy</span><span class=o>.</span><span class=n>settings</span><span class=o>.</span><span class=n>configure</span><span class=p>(</span><span class=n>lm</span><span class=o>=</span><span class=n>lm</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>DocClassificationSignature</span><span class=p>(</span><span class=n>dspy</span><span class=o>.</span><span class=n>Signature</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Classify doc into classes&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>document</span> <span class=o>=</span> <span class=n>dspy</span><span class=o>.</span><span class=n>InputField</span><span class=p>(</span><span class=n>desc</span><span class=o>=</span><span class=s2>&#34;Doc data&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>document_type</span> <span class=o>=</span> <span class=n>dspy</span><span class=o>.</span><span class=n>OutputField</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>desc</span><span class=o>=</span><span class=sa>f</span><span class=s2>&#34;Possible classes: invoice, bank statement, tax form, certificate of liability, other&#34;</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span></code></pre></div><p>I deliberately wrote bad signature so that I could see how it got tuned over time. I could&rsquo;ve speicified Doc as Document, <code>Doc data</code> as <code>Document's OCR Data</code>. After signature, we needed to define a <a href=https://dspy-docs.vercel.app/docs/building-blocks/modules>dspy Module</a>. A dspy module can have one or more dspy signatures. Taking analogy from pytorch, you can think of dspy signatures as <code>nn.Linear</code> that defines the input and output and dspy module as <code>nn.Module</code>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>SimpleDocClassificationModule</span><span class=p>(</span><span class=n>dspy</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>classify_doc</span> <span class=o>=</span> <span class=n>dspy</span><span class=o>.</span><span class=n>Predict</span><span class=p>(</span><span class=n>DocClassificationSignature</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>document</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>classify_doc</span><span class=p>(</span><span class=n>document</span><span class=o>=</span><span class=n>document</span><span class=p>)</span>
</span></span></code></pre></div><p>Lets see how this simple module works, I can invoke this module like this</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>document</span> <span class=o>=</span> <span class=s2>&#34;&#34;&#34;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean rutrum auctor turpis, ac pulvinar sapien pulvinar ac. Sed vehicula nunc ipsum, nec eleifend enim elementum et. Curabitur a odio vel lorem dictum pellentesque. Phasellus ac velit mauris. Cras congue eget quam nec euismod. Curabitur risus est, lobortis id nibh nec, lacinia aliquet purus. Sed ac elit gravida, imperdiet leo sit amet, facilisis ipsum. Praesent gravida, neque a elementum fermentum, ligula lacus imperdiet justo, quis pretium velit tellus ac est. Proin volutpat dui eget leo tincidunt, vitae mattis quam placerat. Curabitur et neque et dolor vulputate feugiat et sit amet massa.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl><span class=n>op</span> <span class=o>=</span> <span class=n>SimpleDocClassificationModule</span><span class=p>()(</span><span class=n>document</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>op</span><span class=p>)</span>
</span></span></code></pre></div><pre tabindex=0><code class=language-output data-lang=output>Prediction(
    document_type=&#39;Other&#39;
)
</code></pre><p>The module sucessfully predicted the right class for the text. Lets inspect the underlying LLM call it made to get the output</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>lm</span><span class=o>.</span><span class=n>inspect_history</span><span class=p>(</span><span class=n>n</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>Classify doc into classes
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>---
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Follow the following format.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Document: Doc data
</span></span><span class=line><span class=cl>Document Type: Possible classes: invoice, bank statement, tax form, certificate of liability, other
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>---
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Document: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean rutrum auctor turpis, ac pulvinar sapien pulvinar ac. Sed vehicula nunc ipsum, nec eleifend enim elementum et. Curabitur a odio vel lorem dictum pellentesque. Phasellus ac velit mauris. Cras congue eget quam nec euismod. Curabitur risus est, lobortis id nibh nec, lacinia aliquet purus. Sed ac elit gravida, imperdiet leo sit amet, facilisis ipsum. Praesent gravida, neque a elementum fermentum, ligula lacus imperdiet justo, quis pretium velit tellus ac est. Proin volutpat dui eget leo tincidunt, vitae mattis quam placerat. Curabitur et neque et dolor vulputate feugiat et sit amet massa.
</span></span><span class=line><span class=cl>Document Type: Other
</span></span></code></pre></div><h2 id=the-tuning>The Tuning<a hidden class=anchor aria-hidden=true href=#the-tuning>#</a></h2><h3 id=dataset>Dataset<a hidden class=anchor aria-hidden=true href=#dataset>#</a></h3><p>To tune the prompt, I needed some examples to use as a training and test set. I resorted to doing what most current research in LLMs does: generating synthetic data from LLMs. Here is a short snippet I used:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>SYSTEM_PROMPT</span> <span class=o>=</span> <span class=s2>&#34;You are a highly intelligent document generator that will generate a dummy ocr text for the provided document type. You will not generate any other text apart from the OCR Text&#34;</span>
</span></span><span class=line><span class=cl><span class=n>classes</span> <span class=o>=</span> <span class=p>[</span><span class=s2>&#34;invoice&#34;</span><span class=p>,</span> <span class=s2>&#34;bank statement&#34;</span><span class=p>,</span> <span class=s2>&#34;tax form&#34;</span><span class=p>,</span> <span class=s2>&#34;certificate of liability&#34;</span><span class=p>,</span> <span class=s2>&#34;other&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>generated_texts</span> <span class=o>=</span> <span class=n>defaultdict</span><span class=p>(</span><span class=nb>list</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>_class</span> <span class=ow>in</span> <span class=n>classes</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=c1># 10 examples per class</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>10</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>completions</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>chat</span><span class=o>.</span><span class=n>completions</span><span class=o>.</span><span class=n>create</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>messages</span><span class=o>=</span><span class=p>[</span>
</span></span><span class=line><span class=cl>                <span class=p>{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;system&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=n>SYSTEM_PROMPT</span><span class=p>},</span>
</span></span><span class=line><span class=cl>                <span class=p>{</span>
</span></span><span class=line><span class=cl>                    <span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                    <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=sa>f</span><span class=s2>&#34;Please generate dummy ocr text for document type: </span><span class=si>{</span><span class=n>_class</span><span class=si>}</span><span class=s2>. Strictly give only the OCR value and no other text&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=p>},</span>
</span></span><span class=line><span class=cl>            <span class=p>],</span>
</span></span><span class=line><span class=cl>            <span class=n>model</span><span class=o>=</span><span class=s2>&#34;gpt-3.5-turbo&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>generated_texts</span><span class=p>[</span><span class=n>_class</span><span class=p>]</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>completions</span><span class=o>.</span><span class=n>choices</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>message</span><span class=o>.</span><span class=n>content</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>df</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>(</span><span class=n>generated_texts</span><span class=p>)</span>
</span></span></code></pre></div><p>In just under 5 minutes (including the time to write the above snippet), I had 50 document ocrs I could use to tune the prompt. What a wonderful era we are living in!</p><p>Now, let&rsquo;s take the 50 samples we generated and prepare them in a format DSPy can consume. For this, we need to convert our data into a list of their data structure <a href=https://dspy-docs.vercel.app/docs/building-blocks/data#dspy-example-objects>dspy.Example</a>, i.e., <code>(List['dspy.Example'])</code>. Then, just like any other supervised learning task, we need to split our dataset into training and testing sets. I chose a 60-40 split—not for any specific reason but just to see how much accuracy gain we can achieve with a small number of training samples. Here is the code to accomplish everything I just mentioned:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>classes</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>columns</span><span class=o>.</span><span class=n>to_list</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>dataset</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>_class</span> <span class=ow>in</span> <span class=n>classes</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>item</span> <span class=ow>in</span> <span class=n>df</span><span class=p>[</span><span class=n>_class</span><span class=p>]</span><span class=o>.</span><span class=n>to_list</span><span class=p>():</span>
</span></span><span class=line><span class=cl>        <span class=n>example</span> <span class=o>=</span> <span class=n>dspy</span><span class=o>.</span><span class=n>Example</span><span class=p>(</span><span class=n>document</span><span class=o>=</span><span class=n>item</span><span class=p>,</span> <span class=n>document_type</span><span class=o>=</span><span class=n>_class</span><span class=p>)</span><span class=o>.</span><span class=n>with_inputs</span><span class=p>(</span><span class=s2>&#34;document&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>dataset</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>example</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>random</span><span class=o>.</span><span class=n>shuffle</span><span class=p>(</span><span class=n>dataset</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>train_set</span> <span class=o>=</span> <span class=n>dataset</span><span class=p>[:</span><span class=mi>30</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>test_set</span> <span class=o>=</span> <span class=n>dataset</span><span class=p>[</span><span class=mi>30</span><span class=p>:]</span>
</span></span></code></pre></div><h3 id=dspy-optimizers-prev-teleprompters>DSPy Optimizers (prev. Teleprompters)<a hidden class=anchor aria-hidden=true href=#dspy-optimizers-prev-teleprompters>#</a></h3><p>Now lets take the above dataset we generated to tune the prompt using DSPy&rsquo;s <a href=https://dspy-docs.vercel.app/docs/building-blocks/optimizers>Optimizers</a>(formerly known as teleprompters, which sounded pretty cool imo). As of this writing, they provide 8 optimizers. They have a good starter <a href=https://dspy-docs.vercel.app/docs/building-blocks/optimizers#which-optimizer-should-i-use>guide</a> regarding which optimizer to use in which cases. Some optimizers add few-shot examples to the prompt, while others optimize the initial prompt, and some do both at the same time.</p><p>I&rsquo;ll be using <a href=https://dspy-docs.vercel.app/docs/building-blocks/optimizers#automatic-instruction-optimization>MIPRO optimizer</a> that will optimize the initial prompt given as well as generate few shot examples. If you want a deeper understanding of how this optimizer works under the hood, DSPy has good deep dive article about <a href=https://dspy-docs.vercel.app/docs/building-blocks/optimizers#automatic-instruction-optimization>how they optimize the prompt</a> and <a href=https://dspy-docs.vercel.app/docs/deep-dive/teleprompter/bootstrap-fewshot>how they generate few shot examples</a>.</p><p>But like any other traditional ML training techniques, we need to first define our evaluation metric/criteria. DSPy allows us to define our own custom function, which will receive ground truth and prediction. We can write any logic of our own and return a <code>bool</code>. For my use case, I used fuzzy matching to check the match between my ground truth and prediction.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>rapidfuzz</span> <span class=kn>import</span> <span class=n>fuzz</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>is_similar</span><span class=p>(</span><span class=n>gold</span><span class=p>:</span><span class=n>dspy</span><span class=o>.</span><span class=n>Example</span><span class=p>,</span> <span class=n>pred</span><span class=p>:</span><span class=n>dspy</span><span class=o>.</span><span class=n>Example</span><span class=p>,</span> <span class=n>trace</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>gt</span> <span class=o>=</span> <span class=n>gold</span><span class=o>.</span><span class=n>document_type</span>
</span></span><span class=line><span class=cl>    <span class=n>prediction</span> <span class=o>=</span> <span class=n>pred</span><span class=o>.</span><span class=n>document_type</span>
</span></span><span class=line><span class=cl>    <span class=n>score</span> <span class=o>=</span> <span class=n>fuzz</span><span class=o>.</span><span class=n>partial_ratio</span><span class=p>(</span><span class=n>gt</span><span class=p>,</span> <span class=n>prediction</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>score</span> <span class=o>&gt;=</span> <span class=mi>80</span>
</span></span></code></pre></div><p>Now, I&rsquo;ll create the optimizer with the metric function I just defined.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>dspy.teleprompt</span> <span class=kn>import</span> <span class=n>MIPRO</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>teleprompter</span> <span class=o>=</span> <span class=n>MIPRO</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>metric</span><span class=o>=</span><span class=n>is_similar</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>verbose</span><span class=o>=</span><span class=kc>True</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><p>Finally, we can start the tunining process by calling <code>teleprompter.compile()</code> with necessary arguments.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>kwargs</span> <span class=o>=</span> <span class=nb>dict</span><span class=p>(</span><span class=n>num_threads</span><span class=o>=</span><span class=mi>4</span><span class=p>,</span> <span class=n>display_progress</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>display_table</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>optimized_module</span> <span class=o>=</span> <span class=n>teleprompter</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>SimpleDocClassificationModule</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>    <span class=n>trainset</span><span class=o>=</span><span class=n>train_set</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>num_trials</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>max_bootstrapped_demos</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>max_labeled_demos</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>eval_kwargs</span><span class=o>=</span><span class=n>kwargs</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><p>Since the optimizers use LLMs internally to tune the prompt and generate few-shot examples, teleprompter.compile() will first provide you with an estimate of the number of LLM calls it will make based on your dataset and the optimizer&rsquo;s parameters that you&rsquo;ve set. <strong>It&rsquo;s important to carefully review this because it&rsquo;s easy to misconfigure a few settings, leading the optimizer to send an astronomically high number of requests to your LLM.</strong></p><div style=text-align:center><img src=/img/dspy/usage.png alt="dspy usage" width=200% height=200%><div style=font-style:italic;color:#555;display:block;margin-top:-4px;margin-bottom:4px;font-size:small>An estimate of LLM calls that the optimizer will make</div></div><p>So for my use case, there will be ~80 LLM calls. That in itself feels like a pretty large number of requests for such a simple usecase and small dataset. I&rsquo;m a bit skeptical about the scalability of this approach as the dataset size grows. However, it is still a much more economically viable option compared to fine-tuning the model itself.</p><p>Once the tuning begins, we can observe how the metrics change after each iteration. Similarly with the debug mode, we can also view the prompts that are getting executed under the hood for tuning.</p><div style=text-align:center><img src=/img/dspy/tuning-debug.png alt="tuning output" width=200% height=200%><div href=# style=font-style:italic;color:#555;display:block;margin-top:-4px;margin-bottom:4px;font-size:small>Example of prompt tuning</div></div><p>Finally, once the tuning is completed, we can take a look into the optimized program.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>optimized_module</span><span class=p>)</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>classify_doc</span> <span class=o>=</span> <span class=n>Predict</span><span class=p>(</span><span class=n>StringSignature</span><span class=p>(</span><span class=n>document</span> <span class=o>-&gt;</span> <span class=n>document_type</span>
</span></span><span class=line><span class=cl>    <span class=n>instructions</span><span class=o>=</span><span class=s1>&#39;Consider semantic features of the document to distinguish between classes&#39;</span>
</span></span><span class=line><span class=cl>    <span class=n>document</span> <span class=o>=</span> <span class=n>Field</span><span class=p>(</span><span class=n>annotation</span><span class=o>=</span><span class=nb>str</span> <span class=n>required</span><span class=o>=</span><span class=kc>True</span> <span class=n>json_schema_extra</span><span class=o>=</span><span class=p>{</span><span class=s1>&#39;desc&#39;</span><span class=p>:</span> <span class=s1>&#39;Doc data&#39;</span><span class=p>,</span> <span class=s1>&#39;__dspy_field_type&#39;</span><span class=p>:</span> <span class=s1>&#39;input&#39;</span><span class=p>,</span> <span class=s1>&#39;prefix&#39;</span><span class=p>:</span> <span class=s1>&#39;Document:&#39;</span><span class=p>})</span>
</span></span><span class=line><span class=cl>    <span class=n>document_type</span> <span class=o>=</span> <span class=n>Field</span><span class=p>(</span><span class=n>annotation</span><span class=o>=</span><span class=nb>str</span> <span class=n>required</span><span class=o>=</span><span class=kc>True</span> <span class=n>json_schema_extra</span><span class=o>=</span><span class=p>{</span><span class=s1>&#39;desc&#39;</span><span class=p>:</span> <span class=s1>&#39;Possible classes: invoice, bank statement, tax form, certificate of liability, other&#39;</span><span class=p>,</span> <span class=s1>&#39;__dspy_field_type&#39;</span><span class=p>:</span> <span class=s1>&#39;output&#39;</span><span class=p>,</span> <span class=s1>&#39;prefix&#39;</span><span class=p>:</span> <span class=s1>&#39;Classification:&#39;</span><span class=p>})</span>
</span></span><span class=line><span class=cl><span class=p>))</span>
</span></span></code></pre></div><p>Just prompting the module doesn&rsquo;t show the few shot examples it has generated. For that, lets predict on the document we defined earlier and inspect the full LM call history to see the optimized prompt and the few shot examples that the optimizer generated.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>document</span> <span class=o>=</span> <span class=s2>&#34;&#34;&#34;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean rutrum auctor turpis, ac pulvinar sapien pulvinar ac. Sed vehicula nunc ipsum, nec eleifend enim elementum et. Curabitur a odio vel lorem dictum pellentesque. Phasellus ac velit mauris. Cras congue eget quam nec euismod. Curabitur risus est, lobortis id nibh nec, lacinia aliquet purus. Sed ac elit gravida, imperdiet leo sit amet, facilisis ipsum. Praesent gravida, neque a elementum fermentum, ligula lacus imperdiet justo, quis pretium velit tellus ac est. Proin volutpat dui eget leo tincidunt, vitae mattis quam placerat. Curabitur et neque et dolor vulputate feugiat et sit amet massa.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl><span class=n>op</span> <span class=o>=</span> <span class=n>optimized_module</span><span class=p>(</span><span class=n>document</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>lm</span><span class=o>.</span><span class=n>inspect_history</span><span class=p>(</span><span class=n>n</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Consider semantic features of the document to distinguish between classes
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>---
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Follow the following format.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Document: Doc data
</span></span><span class=line><span class=cl>Classification: Possible classes: invoice, bank statement, tax form, certificate of liability, other
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>---
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Document: 3047-A Copy C Remove the top KID for your records. SECTION CORPDESCRIPTION $3,719.25 Name: John Doe Address: 123 Main Street, Anytown, USA Account Number: 987654321 FED ID # 12-3456789 Code: ABC123 Do not write or staple in this space 10/15 O Tax year 16 5680 2,347.89 $ Taxpayer must sign and date in blue or black ink, and enter the above information in blue or black ink, then mail to: Department of Revenue, P.O. Box 123, Anytown, USA Income: S Total tax withheld: P KP Total Payments Total from Phaelbe6 Elected to apply to next your 134.65 Underpayment penalty T 2020 Luxury Tax 0.00 48.90 $ Overpaid Refund $2,701.19 Heike Beste This is your tax liability for the tax year shown at the top of the form. Sign and return to the address shown above.
</span></span><span class=line><span class=cl>Classification: tax form
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>---
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Document: OCR Text: Invoice Number: 483921 Date: 09/27/2022 Customer: John Doe Total Amount: $350.00
</span></span><span class=line><span class=cl>Classification: invoice
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>---
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Document: 0498-5672-3498 Name: John Smith SSN: 123-45-6789 Total income: $50,000 Deductions: $10,000 Taxable income: $40,000 Tax owed: $7,000 Please consult your tax advisor for accurate information.
</span></span><span class=line><span class=cl>Classification: tax form
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>---
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Document: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean rutrum auctor turpis, ac pulvinar sapien pulvinar ac. Sed vehicula nunc ipsum, nec eleifend enim elementum et. Curabitur a odio vel lorem dictum pellentesque. Phasellus ac velit mauris. Cras congue eget quam nec euismod. Curabitur risus est, lobortis id nibh nec, lacinia aliquet purus. Sed ac elit gravida, imperdiet leo sit amet, facilisis ipsum. Praesent gravida, neque a elementum fermentum, ligula lacus imperdiet justo, quis pretium velit tellus ac est. Proin volutpat dui eget leo tincidunt, vitae mattis quam placerat. Curabitur et neque et dolor vulputate feugiat et sit amet massa.
</span></span><span class=line><span class=cl>Classification: other
</span></span></code></pre></div><h2 id=evaluation>Evaluation<a hidden class=anchor aria-hidden=true href=#evaluation>#</a></h2><p>Now that we have an optimized module, we can use <a href=https://dspy-docs.vercel.app/docs/building-blocks/metrics>dspy.evaluate.Evaluate</a> to evalute the performance. It&rsquo;s also pretty straightforward to use. We just need to define the evaluator with the desired test set and other configurations. Then, we can simply pass the module and metric function to the evaluator to get the metrics.</p><p>Lets first evaluate our previous module (before optimization).</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>eval_score_old</span> <span class=o>=</span> <span class=n>evaluator</span><span class=p>(</span><span class=n>SimpleDocClassificationModule</span><span class=p>(),</span> <span class=n>metric</span><span class=o>=</span><span class=n>is_similar</span><span class=p>)</span>
</span></span></code></pre></div><div style=text-align:center><img src=/img/dspy/output-1.png alt="unoptimized output" width=200% height=200%><div style=font-style:italic;color:#555;display:block;margin-top:-4px;margin-bottom:4px;font-size:small>Metrics before optimization</div></div><p>We can see that with our initial module, the LLM correctly predicted 17/20 (85%), which is surprisingly good for such bad prompt.</p><p>Now, lets see how our optimized module performed:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>eval_score_new</span> <span class=o>=</span> <span class=n>evaluator</span><span class=p>(</span><span class=n>optimized_module</span><span class=p>,</span> <span class=n>metric</span><span class=o>=</span><span class=n>is_similar</span><span class=p>)</span>
</span></span></code></pre></div><div style=text-align:center><img src=/img/dspy/output-2.png alt="unoptimized output" width=200% height=200%><div style=font-style:italic;color:#555;display:block;margin-top:-4px;margin-bottom:4px;font-size:small>Metrics after optimization</div></div><p>Voila, the metrics improved from 85% to 90% (though it made just one more correct prediction—aah, statistics, how many ways can people find to manipulate you! :P). Though it&rsquo;s not a huge improvement, please note that the data was dummy data. We improved the prompt and added a few-shot example in an algorithmic manner rather than begging the LLM to respond correctly, which I feel is pretty neat!</p></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/llm/>Llm</a></li><li><a href=http://localhost:1313/tags/prompt-engineering/>Prompt-Engineering</a></li><li><a href=http://localhost:1313/tags/python/>Python</a></li></ul><nav class=paginav><a class=prev href=http://localhost:1313/posts/how-decorator-broke-my-app/><span class=title>« Prev</span><br><span>How a Decorator Crashed My Flask App: Lessons Learned</span>
</a><a class=next href=http://localhost:1313/posts/getting-started/><span class=title>Next »</span><br><span>Getting Started ... Again</span></a></nav></footer><script src=https://giscus.app/client.js data-repo=suyogdahal/suyogdahal.github.io data-repo-id=R_kgDOMFtnVQ data-category=General data-category-id=DIC_kwDOMFtnVc4CgAkO data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=en data-loading=lazy crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2024 <a href=http://localhost:1313/>Suyog's personal blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>